{"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport bz2\nimport pickle\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as plt\nimport json\n#get_ipython().run_line_magic('matplotlib', 'inline')\nfrom tensorflow.keras.optimizers import Adam\n\n# In[3]:\ntrainfile = bz2.BZ2File('../input/amazonreviews/train.ft.txt.bz2','r')\nlines = trainfile.readlines()\n\nsent_analysis = []\ndef sent_list(docs,splitStr='__label__'):\n    for i in range(1,len(docs)):\n        text=str(lines[i])\n        splitText=text.split(splitStr)\n        #print(i)\n        secHalf=splitText[1]\n        text=secHalf[2:len(secHalf)-1]\n        sentiment=secHalf[0]\n        sent_analysis.append([text,sentiment])\n    return sent_analysis\n\nsentiment_list=sent_list(lines[:1000000],splitStr='__label__')\n\ntrain_df = pd.DataFrame(sentiment_list,columns=['Text','Sentiment'])\n\ndata_train=train_df[:20000]\ndata_test=train_df[50000:55000]\n\n#a=input('path of the taining dataset with fields as title and tag(0,1) ')\n#b=input('path of test dataset')\n#data_train=pd.read_csv('../input/kuc-hackathon-winter-2018/drugsComTrain_raw.csv')\n\n\n# In[4]:\n\n\n#data_train\n\n\n# In[5]:\n\n\n#data_train['review'][7]\n\n\n# In[6]:\n\n\n#data_test=pd.read_csv('../input/kuc-hackathon-winter-2018/drugsComTest_raw.csv')\n\n\n# In[7]:\n\n\n#data_test\n#data_train=data_train[:10000]\n#data_test=data_test[:3000]\n\n# In[8]:\ndata_train.rename(columns={'Text':'title','Sentiment':'tag'},inplace=True)\ndata_test.rename(columns={'Text':'title','Sentiment':'tag'},inplace=True)\n\n#data_train['rating'].value_counts()\n#print('training_dataset',data_train)\n#print('training_dataset',data_test)\n\n# In[9]:\n\n\ndef make_tags(x):   #converting the ratings column into 0's and 1's.  for binary classifier to take place\n    if(x==\"1\"):\n        return 0\n    else:\n        return 1\n  \n\n\n# In[10]:\n\n\ndata_train['tag']=data_train['tag'].apply(lambda x: make_tags(x))\ndata_test['tag']=data_test['tag'].apply(lambda x: make_tags(x))\n#print(data_train)\n\n# In[11]:\n\n\n#data_train\n\n\n# In[12]:\n\n#data_train.rename(columns={'review':'title'},inplace=True)\n#data_test.rename(columns={'review':'title'},inplace=True)\n\n#applying sentence tokenizer\nimport nltk.data \ntokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n# Loading PunktSentenceTokenizer using English pickle file \ndef make_sent_token(x):\n    return tokenizer.tokenize(x) \n#converting each paragraph into separate sentences\n\n\n# In[13]:\n\n\ndata_train['sentence_token']=data_train['title'].apply(lambda x: make_sent_token(x))\n\n\n# In[14]:\n\n\ndata_test['sentence_token']=data_test['title'].apply(lambda x: make_sent_token(x))\n\n\n# In[15]:\n\n\n#data_train.drop(columns=['uniqueID','date','usefulCount','condition','drugName'],inplace=True,axis=1)# dropping irrelevant columns\n\n\n# In[16]:\n\n\n#data_test.drop(columns=['uniqueID','date','usefulCount','condition','drugName'],inplace=True,axis=1)\n\n\n# In[17]:\n\n\n#data_train\n\n\n# In[18]:\n\n\ndata_train['no_of_sentences']=data_train['sentence_token'].apply(lambda x:len(x))\n\n\n# In[19]:\n\n\ndata_test['no_of_sentences']=data_test['sentence_token'].apply(lambda x:len(x))\n\n\n# In[20]:\n\n\n#max(data_train['no_of_sentences'])##no of rows in sentence matrix which is to be feed in model(max number of sentence in any paragraph)\n\n\n# In[21]:\n\n\n#len(data_train[data_train['no_of_sentences']==92]['review'])\n\n\n# In[22]:\n\n\n#max(data_test['no_of_sentences'])\n\n\n# In[23]:\n\n\ndef max_length_of_sentence(x,y):\n    sen=x\n    nu=y\n    #print(sen)\n    ma=0\n    if(nu>1):\n        l=sen.split('.')\n        #print(l)\n        for i in range(len(l)):\n            k=l[i].replace(',','')\n            maxi=len(k.split())\n            #print(maxi)\n            if(maxi>ma):\n                ma=maxi\n        return ma\n    else:\n        return len(sen.split())\n        \n    \n\n\n# In[24]:\n\n\ndata_train['max_words_in_sentence']=data_train.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)\n\n\n# In[25]:\n\n\ndata_test['max_words_in_sentence']=data_test.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)\n\n\n# In[26]:\n\n\n#max(data_train['max_words_in_sentence'])## number of columns in the data to be feeded\n\n\n# In[27]:\n\nx1=max(data_train['no_of_sentences'])\ny1=max(data_train['max_words_in_sentence'])\n\nx2=max(data_test['no_of_sentences'])\ny2=max(data_test['max_words_in_sentence'])\n\nif(x1>=x2):\n    m=x1\nelse:\n    m=x2\n    \nif(y1>=y2):\n    n=y1\nelse:\n    n=y2\n\n#So each para will be converted to a m*n matrix\n\nprint('x1,x2,y1,y2',x1,x2,y1,y2)\n#So each para will be converted to a m*n matrix\n\n\n# In[28]:\n\n\n\n\n# # Major part starts here ..... Now converting the paragraph into required matrix\n\n# In[29]:\n\n\nimport re\nimport string \nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\ndef make_tokens(text):     ##Converting into single tokens in order to create the vocabulary\n    return word_tokenize(text)\n\n\ndata_train['tokens']=data_train['title'].apply(lambda x: make_tokens(x))\ndata_test['tokens']=data_test['title'].apply(lambda x: make_tokens(x))\n\n\n# In[30]:\n\n\n#data_train['tokens']\n\n\n# In[ ]:\n\n\n#from gensim import models\n#word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n#word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n\n\n\nembeddings_index = {}\nf = open('../input/glove6b300dtxt/glove.6B.300d.txt')\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n    embeddings_index[word] = coefs\nf.close()\n\nprint('GloVe data loaded')\n\n# In[ ]:\n\n\nall_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\ntraining_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))\npara_max=max(training_sentence_lengths)\n\n\n# In[ ]:\n\n\n#len(TRAINING_VOCAB)\n\n\n# In[ ]:\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), char_level=False)\ntokenizer.fit_on_texts(data_train['title'])       # we assigned values \n\n\n# In[ ]:\n\n\ntrain_word_index = tokenizer.word_index\n\n\n# In[ ]:\n\n\n#print(train_word_index)\n\n\n# In[ ]:\n\n\n#data_train.to_csv('medic_train.csv')\n#data_test.to_csv('medic_test.csv')\n\n\n# In[ ]:\n\n\ndef make_train_seq(x):\n    return tokenizer.texts_to_sequences(x)\ndata_train['train_seq']=data_train['sentence_token'].apply(lambda x:make_train_seq(x) )\ndata_test['train_seq']=data_test['sentence_token'].apply(lambda x:make_train_seq(x) )\n\n\n# In[ ]:\n\n\n#(data_train['train_seq'])   # here every para has been encoded\n\n\n# In[ ]:\n#print(data_train)\n\n\n\n\n# In[ ]:\n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ndef padding(x):    #now padding each sentence to a length of n...number of columns\n    MAX_SENTENCE_LENGTH=n  #(no of columns)\n    return pad_sequences(x,maxlen=MAX_SENTENCE_LENGTH,padding='post')\n\ndata_train['padded']=data_train['train_seq'].apply(lambda x:padding(x))\ndata_test['padded']=data_test['train_seq'].apply(lambda x:padding(x))\n\n\n# In[ ]:\n\n\n#(data_train.padded[8])\n\n\n# In[ ]:\n\n\n\n## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n# prepare embedding matrix \nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.initializers import Constant\n\n## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\nEMBEDDING_DIM = embeddings_index.get('a').shape[0]\nprint(EMBEDDING_DIM)\n#num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n #= np.zeros(len(train_word_index) + 1, EMBEDDING_DIM)\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, \n EMBEDDING_DIM))\nfor word, i in train_word_index.items():\n    #print(\"sd\")\n    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n    if embedding_vector is not None:\n        train_embedding_weights[i] = embedding_vector\nprint(train_embedding_weights.shape)\n        # words not found in embedding index will be all-zeros.\n        \n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\n#embedding_layer = Embedding(num_words,\n                          #  EMBEDDING_DIM,\n                          #  embeddings_initializer=Constant(embedding_matrix),\n                          #  input_length=MAX_SEQUENCE_LENGTH,\n                          #  trainable=False)\n\n\n#EMBEDDING_DIM=300\n#train_embedding_weights = np.zeros((len(train_word_index)+1, \n #EMBEDDING_DIM))\n#for word,index in train_word_index.items():\n #train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n#print(train_embedding_weights.shape)\n\n\n# In[43]:\n\n\ndef make_full_para(x):     #92 cross 192 matrix of a paragraph.   (m*n)\n    l=len(x)\n    h=m-l    #no. of extra rows to be added\n    z=[0]*h*n       #1D vector(#addding extra lines for zeroes as padding)\n    z=np.reshape(z,(h,n))    #reshaping it to match the dimension of paragraph\n    s=x.tolist()+z.tolist()\n    return s \n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\ndata_train['full_para']=data_train['padded'].apply(lambda x : make_full_para(x))\ndata_test['full_para']=data_test['padded'].apply(lambda x : make_full_para(x))\n\n\n# In[ ]:\n\n\n#data_train.full_para\n\n\n# In[ ]:\n\n\ndef create_1d_para(x):\n    l=[]\n    for i in x:\n        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n    return l\n        \n    \n\n\n# In[ ]:\n\ndata_train['single_d_array']=data_train['full_para'].apply(lambda x: create_1d_para(x) )\ndata_test['single_d_array']=data_test['full_para'].apply(lambda x: create_1d_para(x) )\n\n\n# In[ ]:\n\n\n#train_cnn_data=np.array(data_train['single_d_array'].tolist())\n\n\n# In[ ]:\n\n\ntrain_cnn_data=np.array(data_train['single_d_array'].tolist())\ntest_cnn_data=np.array(data_test['single_d_array'].tolist())\n\n\n# In[ ]:\n\n\ny_train=data_train['tag'].values\n#y_test=data_test['tag'].values\n\n\n# In[ ]:\n\nprint('Startting the training')\n#from __future__ import print_function\nfrom tensorflow.keras.layers import Embedding\n\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np\n\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM,SpatialDropout1D,Reshape\nfrom tensorflow.keras.layers import Embedding,concatenate\nfrom tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D,MaxPool2D,MaxPool3D,GlobalAveragePooling2D,Conv3D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\n\n\n# In[ ]:\n\n\nfilter_sizes = [1,2,3,4]\nnum_filters = 32\nembed_size=300\nembedding_matrix=train_embedding_weights\nmax_features=len(train_word_index)+1\nmaxlen=m*n\ndef get_model():    \n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = SpatialDropout1D(0.4)(x)\n    x = Reshape((m, n, 300))(x)\n    #print(x)\n    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n                                                                                    activation='relu')(x)\n    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3),\n                                                                                    activation='relu')(x)\n    \n    \n    \n    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 1), \n                                                                                    activation='relu')(x)\n    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2), activation='relu')(x)\n    \n    \n    \n    \n    maxpool_0 = MaxPool2D()(conv_0)\n    maxpool_0=Flatten()(maxpool_0)\n    maxpool_1 = MaxPool2D()(conv_1)\n    maxpool_1=Flatten()(maxpool_1)\n    #maxpool_2 = MaxPool2D()(conv_2)\n    #maxpool_3 = MaxPool2D()(conv_3)\n    \n    maxpool_4 = MaxPool2D()(conv_4)\n    maxpool_4=Flatten()(maxpool_4)\n    maxpool_5 = MaxPool2D()(conv_5)\n    maxpool_5=Flatten()(maxpool_5)\n    #maxpool_6 = MaxPool2D()(conv_6)\n    #maxpool_6=Flatten()(maxpool_6)\n    #maxpool_7 = MaxPool2D()(conv_7)\n   # maxpool_7=Flatten()(maxpool_7)\n        \n    w=concatenate([maxpool_4, maxpool_5],axis=1)    \n    z = concatenate([maxpool_0, maxpool_1],axis=1)\n    \n    z = Flatten()(z)\n    z=concatenate([w,z],axis=1)\n    z=Dense(units=64,activation=\"relu\")(z)\n    z = Dropout(0.5)(z)\n        \n    outp = Dense(1, activation=\"sigmoid\")(z)\n    \n    model = Model(inputs=inp, outputs=outp)\n    \n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n    return model\n\n\n# In[ ]:\n\n\nmodel=get_model()\n\n\n# In[ ]:\n\n\nprint(model.summary())\n\n\n# In[ ]:\n\n\n\n#define callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]\nhist = model.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n\n\nPkl_Filename = \"Pickle_sentiment_Model.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(model, file)\n# In[ ]:\n\n\npred=model.predict(test_cnn_data)\ny_test=pred\ny_test=y_test.tolist()\noutput_class_pred=[]\nfor i in range(len(y_test)):\n    if(y_test[i][0]<0.5):\n        output_class_pred.append(0)\n    else:\n        output_class_pred.append(1)\n        \noriginal_ans=data_test['tag']\noriginal_ans=original_ans.tolist()\n\n\n# In[ ]:\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#as its a fake news classifier , so identifying a fake class will be a TP\ndef check_metric(output_class_pred,original_ans):\n    rightly_predicted=0\n    TP=0\n    for i in range(len(y_test)):\n        if(original_ans[i]==output_class_pred[i]):\n            rightly_predicted+=1\n        \n        \n    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n    print('TP',TP)\n    accuracy=rightly_predicted/len(y_test)\n    print(classification_report(original_ans,output_class_pred))\n    print(confusion_matrix(original_ans,output_class_pred))\n    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n    \n    precision=TP/(TP+FP)\n    recalll=TP/(FN+TP)\n    F1=2*precision*recalll/(precision+recalll)\n    sensiti=TP/(TP+FN)\n    specifici=TN/(TN+FP)\n    numerator=TP*TN - FP*FN\n    \n    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n    MCc=numerator/denominator\n    G_mean1=np.sqrt(sensiti*precision)\n    G_mean2=np.sqrt(sensiti*specifici)\n    print('precision:' ,TP/(TP+FP))\n    print('recall:',TP/(FN+TP))\n    print(\"F1:\",F1)\n    print(\"Specificity:\",TN/(TN+FP))\n    print(\"Sensitivity \",TP/(TP+FN))\n    print('G-mean1:',np.sqrt(sensiti*precision))\n    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n    print(\"MCC :\",MCc)\n    acc=[]\n    pre=[]\n    recall=[]\n    f1=[]\n    specificity=[]\n    sensitivity=[]\n    GMean1=[]\n    Gmean2=[]\n    MCC=[]\n    tp=[]\n    fp=[]\n    fn=[]\n    tn=[]\n    acc.append(accuracy)\n    pre.append(precision)\n    recall.append(recalll)\n    f1.append(F1)\n    specificity.append(specifici)\n    sensitivity.append(sensiti)\n    GMean1.append(G_mean1)\n    Gmean2.append(G_mean2)\n    MCC.append(MCc)\n    tp.append(TP)\n    fp.append(FP)\n    tn.append(TN)\n    fn.append(FN)\n    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n    metric=pd.DataFrame(data)\n    return metric\n    \n    \n    \n\n\n# In[ ]:\n\n\nresi=check_metric(output_class_pred,original_ans)\n\n\n# In[ ]:\n\n\nresi.to_csv('results.csv', mode='w', index = False, header=resi.columns,columns=resi.columns)\n'''\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\n## now perparing training data for yoon kim model\n\n\n# In[ ]:\n\n\ndef create_single_line_para(x):\n    l=[]\n    for i in x:\n        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n    return l\n        \n\n\n# In[ ]:\n\n\ndata_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\ndata_test['create_single_line_para']=data_test['train_seq'].apply(lambda x: create_single_line_para(x) )\n\n\n# In[ ]:\n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nyoon_kim_train_data=np.array(data_train['create_single_line_para'].tolist())\nyoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')\n\n# In[ ]:\nyoon_kim_test_data=np.array(data_test['create_single_line_para'].tolist())\nyoon_kim_test_data=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n\n\n#from __future__ import print_function\nfrom tensorflow.keras.layers import Embedding\n\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np\n\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\nfrom tensorflow.keras.layers import Embedding,concatenate\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\n\n\n# In[ ]:\n\n\ntrain_y=pd.get_dummies(y_train)\n\n\n# In[ ]:\n\n\ntrains_y=train_y[[0,1]].values\n\n\n# In[ ]:\n\n\nembed_size=300\nembedding_matrix=train_embedding_weights\nmax_features=len(train_word_index)+1\nmaxlen=para_max \nmax_sequence_length=para_max\nMAX_SEQUENCE_LENGTH=para_max\nEMBEDDING_DIM=300\n\n\n#model3 yoon kim\n\n\n# In[ ]:\n\n\ndef ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n    \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=trainable)\n\n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n    convs = []\n    filter_sizes = [3,4,5]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n        convs.append(l_pool)\n\n    l_merge = concatenate(convs, axis=1)\n\n    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n    #pool = MaxPooling1D(pool_size=2)(conv)\n\n    #if extra_conv==True:\n        #x = Dropout(0.01)(l_merge)  \n    #else:\n        # Original Yoon Kim model\n        #x = Dropout(0.001)(pool)\n    x = Flatten()(l_merge)\n    \n    x = Dropout(0.5)(x)\n    # Finally, we feed the output into a Sigmoid layer.\n    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n    preds = Dense(2, activation='softmax')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model\n\n\n# In[ ]:\n\n\nmodel1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n                 True)\n\n\n# In[ ]:\n\n\ntraining_data=yoon_kim_train_data\n\n\n# In[ ]:\n\n\ntesting_data=yoon_kim_test_data\n\n\n# In[ ]:\n\n\n\n#define callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]\nhist = model1.fit(training_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n\n\n# In[ ]:\n\n\npred=model1.predict(testing_data)\ny_test=pred\ny_test=y_test.tolist()\noutput_class_pred=[]\n#output_class_pred=[]\nfor i in range(len(y_test)):\n    m=max(y_test[i])\n    if(y_test[i].index(m)==0):\n        output_class_pred.append(0)\n    else:\n        output_class_pred.append(1)\n        \n        \noriginal_ans=data_test['tag']\noriginal_ans=original_ans.tolist()\n\n\n# In[ ]:\n\n\n#as its a fake news classifier , so identifying a fake class will be a TP\ndef check_metric(output_class_pred,original_ans):\n    rightly_predicted=0\n    TP=0\n    for i in range(len(y_test)):\n        if(original_ans[i]==output_class_pred[i]):\n            rightly_predicted+=1\n        \n        \n    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n    print('TP',TP)\n    accuracy=rightly_predicted/len(y_test)\n    print(classification_report(original_ans,output_class_pred))\n    print(confusion_matrix(original_ans,output_class_pred))\n    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n    \n    precision=TP/(TP+FP)\n    recalll=TP/(FN+TP)\n    F1=2*precision*recalll/(precision+recalll)\n    sensiti=TP/(TP+FN)\n    specifici=TN/(TN+FP)\n    numerator=TP*TN - FP*FN\n    \n    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n    MCc=numerator/denominator\n    G_mean1=np.sqrt(sensiti*precision)\n    G_mean2=np.sqrt(sensiti*specifici)\n    print('precision:' ,TP/(TP+FP))\n    print('recall:',TP/(FN+TP))\n    print(\"F1:\",F1)\n    print(\"Specificity:\",TN/(TN+FP))\n    print(\"Sensitivity \",TP/(TP+FN))\n    print('G-mean1:',np.sqrt(sensiti*precision))\n    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n    print(\"MCC :\",MCc)\n    acc=[]\n    pre=[]\n    recall=[]\n    f1=[]\n    specificity=[]\n    sensitivity=[]\n    GMean1=[]\n    Gmean2=[]\n    MCC=[]\n    tp=[]\n    fp=[]\n    fn=[]\n    tn=[]\n    acc.append(accuracy)\n    pre.append(precision)\n    recall.append(recalll)\n    f1.append(F1)\n    specificity.append(specifici)\n    sensitivity.append(sensiti)\n    GMean1.append(G_mean1)\n    Gmean2.append(G_mean2)\n    MCC.append(MCc)\n    tp.append(TP)\n    fp.append(FP)\n    tn.append(TN)\n    fn.append(FN)\n    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n    metric=pd.DataFrame(data)\n    return metric\n    \n    \n    \n\n\n# In[ ]:\n\n\nresi=check_metric(output_class_pred,original_ans)\n\n\n# In[ ]:\n\n\nresi.to_csv('results.csv', mode='a', index = False, header=resi.columns,columns=resi.columns)\n'''\n","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}